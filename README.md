# CLAR
CLAR is a novel Communication Learning mechanism of multi-Agent Reinforcement learning (CLAR) for heterogeneous scenarios. The above pictures shows some experiment details for CLAR.
## PCP
Predator-Capture-Prey consists of two types of cooperative agents, predators and captures, as shown in `PCP.png`. The target of the predator agent is to find the prey. At each time step, the state space of all predator agents is a concate-nation vector, which denotes the position of the agent and the information indicating the existence of prey, other predator agents or other capture agents. The dimension of action space of the predator agent is five and the action space of all agents is the same, which consists of action move down, move up, move left, move right and stay. The second type of agent, the capture agent, aims to locate and capture prey. The capture agent differs from the predator agent in both the action space and the observation space in that the capture agent does not obtain any observation in-puts from the environment. In addition, the capture agents have an additional action capture in their action space, and when they move to the location of the prey, they take the action capture to capture the prey located in the corresponding grid. Therefore, the target of the game is that all predators find a fixed prey, all captures find prey and then capture it. Each predator receives a -0.05 penalty per time-step until it accomplishes its per-class.


## SMAC
We evaluate it on the SMAC  benchmark and in-crease the difficulty of cooperation. On the one hand, we reduce the agent's range of sight from 9 to 2, and on the other hand, we choose challenging maps with compli-cated terrain as shown in `SMAC.png`.  In different combat scenarios, Ally units are all con-trolled by the reinforcement learning agents and enemy units are mastered by the built-in AI, whose level of dif-ficulty is set to medium. Enemy units and ally units can be asymmetrical, and their initial positions are random.
The action space of the agent is of dimension four, which consists of actions move, noop, attack, and stop. Agents attack and move in these maps of continuous space under the control of these actions. At each time step, the agent receives a reward that is equivalent to the cumulative damage done to the enemy units. The agent can obtain an additional bonus of 10 for each enemy unit it kills and 200 for each battle it wins. We evaluate the proposed method on the three scenarios shown in `MAPS.png`, which provides brief descriptions of these scenarios. 
